{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Scalar's and Vector's\nScalar's are entities that can be measured using a single real number. Examples are height, weight , temperature etc.\nAn entity which has both magnitude and direction are called Vectors. Force, velocity, movement in a stock price etc , all can be represented as vectors as besides just the quantity, the direction in which these are acting are crucial for their complete definition."
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "The development of algebra of vectors have been around since beginning of 20th century,but the concept of them have been around even hundreds of years before them. Geometrically, vectors are shown with an arrow connecting two points , the the direction of the arrow representing the direction."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![vector.png](attachment:vector.png)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Vectors with same length and direction are said to be equal, meaning just being at a different position in a coordinate system does not affect the vectors. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "A vector whose starting an ending point coincides are called zero vectors, and they are assumed to be in any direction."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##### Why are vectors useful ?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Vectors can be used to represent entities and model various phenomenon, physical or logical.  \nHere are some examples:  \n1) In physics, vectors in 4-dimensions can be used to denote spacetime where the first 3 vectors are 3-d coordinates of space and 4th vector is for time.  \n2) In machine learning, we typically represent fetaures in the data as vectors. For example, In the dataset containing height and weight details for man and women, each data point is considered as vector with components in height and weight.  \n3) In studying aerodynamics, the forces like velocity , lift and drag are all treated as vectors as  their directions are equally important as their magnitudes.  \n  \nThese are just few examples, but the point to drive home is that when we start to model a phenomenon, we start by identying the components within that system and vectors are popular and convinient way of think about those components. Once we have identified the vectors, we then have the whole vector algebra which has evolved over centuries at our disposal to study interactions between these systems."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "While the mathematical notation of vectors is more elaborate, we can represent a vector in numpy by using a single dimensional array. When we represent vectors programatically, we will think that all the vectors are originating from origin and hence can be represented by only the end-coordinates."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "[fig-vector_representation]"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nvector_2d = np.array([1,1]) \nvector_3d = np.array([1,1,1]) ",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##### Length of a vector"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "u = np.array([1,1])\nv = np.array([2,1])",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "[fig-length]"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "len_u = np.sqrt(np.sum(np.square(u)))\nlen_v = np.sqrt(np.sum(np.square(v)))\nprint('||u|| :'+str(len_u))\nprint('||v|| :'+str(len_v))",
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "||u|| :1.41421356237\n||v|| :2.2360679775\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##### Algebraic properties"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Operations are how the variables interact. Addition, subtraction, multiplication etc are common operations that we learn to first to apply on real numbers and then later to algebraic variables.  The nature of these operators can be described using properties that these operators exhibit. Let's see some basic properties and how these work on real numbers:  \n**Associativity:** Allows you to group the variables differently without having any effect. For example:  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2 + (3 + 4) =  (2 + 3) + 4  or  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2 x (3 x 4)  = (2 x 3) x 4\nSo we say that + and x are associative.    \nHowever, 2 - (3 - 2) != (2 - 3)- 2 and 1/(2/3) != (1/2)/3 , therefore substraction and division are not associative.  \n**Commutativity:** This property allows to interchange the order of the variables without having any effect.Example  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2+3=3+2 and  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2x3=3x2  \nbut 2-3 != 3-2 and 2/3 != 3/2. So again, addition and multiplication are commutative while substraction and divison are not.  \n**Identity and Inverse:** Identity property means having a member such that applying the operator to any variable does not change the variable. For example, 0 is an identity under addition operation while 1 is the identity under multiplication. Inverse property means having a member such that applying the operator produces the identity. For example, in addition for every x, adding -x will result in 0, which is the identity.  \nThink of other operators like exponentiation in terms of above properties.  \nAlso there are lot more properties that can be used to characterize an operator like transitivity, reflexivity, distributivity etc."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Algebraic Operations on vectors"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##### Addition"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If ***u*** and ***v*** are two vectors, the sum can be represented as arrow from initial point of v to terminal point of w."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![vector_sum.png](attachment:vector_sum.png)"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "u = np.array([1,1])\nv = np.array([2,1])\nprint(u+v)",
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[3 2]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Do note that the vectors have to be of same dimensions for this addition to make sense. Let's try to do the addition when the vectors are of different dimensions."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "a = np.array([1,1,1])\nb = np.array([2,1])\nprint(a+b)",
      "execution_count": 19,
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (3,) (2,) ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-a36cfdae783c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,) (2,) "
          ]
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Addition with vectors are both associative and commutative."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##### Scalar Multiplication"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can multiply vectors by scalers , which scales the magnitude of the vectors. Scaling with negative quantity scalers revers the direction too."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "[fig-vector scaling]"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "u = np.array([1,1])\nprint(3*u)",
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[3 3]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "scalar vector multiplication is distributive over scalar addition\n\n(a+b)x**u** = ax**u**+bx**u**"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(5*u 3*u+2*u))\nprint(5*u 3*u+2*u))",
      "execution_count": 21,
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-21-e8d88c926edc>, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-e8d88c926edc>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print(5*u 3*u+2*u))\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "### Dot Product\nThe dot product is very important operator for vectors. It is also called inner product.It is denoted by '**.**' symbol.  \nAngle between two vectors:"
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "![dot.png](attachment:dot.png)"
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "The angle between the two vectors **u** and **v** is given by:\n$$ u.v = ||u||\\ ||v|| \\ cos \\theta$$\nFrom the above equation, if theta is 90 degrees, i.e,if **u** and **v** are perpendicular, then cos90 =0 and hence the dot product is also 0. To re-iterate:\n$$ u.v = 0 \\ if \\ \\theta=90^{\\circ}$$\nFundamentally, dot product is a projection. The dot product **u.v** gives the projection of **u** along **v**."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![dot.png](attachment:dot.png)"
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "Dot product as rectangular coordinates:  \nLet the components of both U and V along the orthogonal basis vectors be (Ux,Uy) and (Vx,Vy) respectively.Hence, the dot product can now be written as :  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**u . v** = ( ux **i** + uy **j** ).( vx **i** +vy **j** )  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= ux.vx **i** + uy.vx **i** **j** + ux **i** vy **j** + uy.vy **j**   \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= ux.vx+uy.vy  \ni.e , the dot product of two vectors is same as product of it's corresponding components.  \nAn important thing to keep in mind is vectors can exists without any coordinate system, hence above representation is just for convinience under certain scenarios.\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The dot product of vectors produces a scalar output. What dot product gives you can be thought in various ways, think of it as a measure of similarity between two vectors, or by measure of contribution of a vector to another.  \nThe dot product are present everywhere in physics and computing is either as direct form or indirect forms like matrix multiplication. Wikipedia has some pointers to uses:\nhttps://en.wikipedia.org/wiki/Dot_product#Physics"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "u=np.array([1,2,3])\nv=np.array([1,2,3])\nz=u.dot(v)\nprint(z)",
      "execution_count": 42,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "14\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##### Normal Vectors\nVectors of length 1 are called normal vectors.They are also known as unit vectors. Any vector can be normalized by diving the individual components by it's length.  \n$$ N = \\frac{X}{||X||} $$"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(u/np.linalg.norm(u))\nprint(\"Length after nomalization: \",0.26726124**2+0.53452248**2+0.80178373**2)",
      "execution_count": 34,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[ 0.26726124  0.53452248  0.80178373]\nLength after nomalization:  1.0000000017244008\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "### Dot product for identical vectors\nu=np.array([1,2,3])\nv=np.array([1,2,3])\nu_norm = u/np.linalg.norm(u)\nv_norm = v/np.linalg.norm(v)\nprint(u_norm.dot(v_norm))",
      "execution_count": 35,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "1.0\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "### Dot product for different vectors\nu=np.array([1,2,3])\nv=np.array([4,9,1])\nu_norm = u/np.linalg.norm(u)\nv_norm = v/np.linalg.norm(v)\nprint(u_norm.dot(v_norm))",
      "execution_count": 40,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "0.674936558945\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "### Dot product for perpendicular vectors\nu=np.array([1,0,0])\nv=np.array([0,1,0])\nu_norm = u/np.linalg.norm(u)\nv_norm = v/np.linalg.norm(v)\nprint(u_norm.dot(v_norm))",
      "execution_count": 41,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "0.0\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Linear Combinations"
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "Let's **v1,v2,v3....vn** be vectors. Linear combination is defined as "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a**v1** + b**v2** + c**v3** + ..... + n**vn**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Example: One Linear combination of [2,3,4] and [3,4,5] is"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "5 x [2,3,4] + 2 x [3,4,5]"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "= [10,15,20] + [6,8,10]"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "= [16,23,30]"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Example2: Portfolio\nAssume that you have a quantities of stock1 and b quantities of stock2. Then the toal value of your portfolio will be:  \na x stock1 + b x stock2"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Linear Equations\nA linear equation is an equation of the form  \n$$ a_1x_1+a_2x_2+a_3x_3+.....+a_1x_n = c_1 $$\n where  there are n variables  $$ x_1,x_2 ... x_n $$ and $$a_1,a_2 .. c_1 $$ are known constant  \n So the solution of the equations is ordered list of numbers s1,s2,s3...sn that when substituted satisfies the linear equation.  \n\n##### System of linear equations\nThe graph of a linear equations is a line. In a system of linear equations, there are multiple lines and an interesting question to ask for is where does the line intersect. In order words, what are the solutions which satisfies all the linear equations in a system of equations. There can be 3 possible answers to above questions:  \n1) The lines are parallel .. they do not intersect and thus have no solution.Such systems are also called singular or inconsistent.  \n2) The lines are same .. there are infinite number of solutions  \n3) The lines intersect have exactly one solution  "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Linear Transformations\nA linear transformation, T:U→V, is a function that carries elements of the vector space U (domain) to the vector space \nV (codomain), and which has two additional properties  \n$$ T(u_1+u_2)=T(u_1)+T(u_2) \\ for \\ all \\ u_1,u_2∈U  $$  \nand\n$$ T(αu)=αT(u) \\ for \\ all \\ u∈U \\ and \\ all \\ α∈C  $$ \nAll it's trying to say is, with linear transformations,  you should:  \n1) Be able to transforms individual components first and then add them, or add them first and then apply the transformation without any side effect   \n2) Scale the vector first and then apply the transformation or apply the transformation and then scale the vector without making any difference"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Examples"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's take a linear transform function\n$$ T(x_1,x_2) =  (\\begin{array}{c} x_1 + 2*x_2 \\\\ x_2  \\end{array} ) $$"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "### Implementing transform function (x1,x2) -> (x1+2x2,x2)\ndef linearTransform(vector2d):\n    return np.array([vector2d[0]+2*vector2d[1],vector2d[1]])",
      "execution_count": 131,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "u = np.array([1,1])\nv = np.array([2,2])\nprint(\"========Property 1:========\")\nprint(\"T(u1+u2)\")\nprint(linearTransform(u+v))\nprint(\"T(u1)+T(u2)\")\nprint(linearTransform(u)+linearTransform(v))\nprint(\"========Property 2:========\")\nprint(\"T(au1)\")\nprint(linearTransform(5*u))\nprint(\"aT(u1)\")\nprint(5*linearTransform(u))",
      "execution_count": 113,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "========Property 1:========\nT(u1+u2)\n[9 3]\nT(u1)+T(u2)\n[9 3]\n========Property 2:========\nT(au1)\n[15  5]\naT(u1)\n[15  5]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's look at a non-linear transform:\n$$ T(x_1,x_2) =  (\\begin{array}{c} x_1-1 \\\\ x_2  \\end{array} ) $$"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "### Implementing transform function (x1,x2) -> (x1-1,x2)\ndef nonLinearTransform(vector2d):\n    return np.array([vector2d[0]-1,vector2d[1]])",
      "execution_count": 114,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "u = np.array([1,1])\nv = np.array([2,2])\nprint(\"========Property 1:========\")\nprint(\"T(u1+u2)\")\nprint(nonLinearTransform(u+v))\nprint(\"T(u1)+T(u2)\")\nprint(nonLinearTransform(u)+nonLinearTransform(v))\nprint(\"========Property 2:========\")\nprint(\"T(au1)\")\nprint(nonLinearTransform(5*u))\nprint(\"aT(u1)\")\nprint(5*nonLinearTransform(u))",
      "execution_count": 130,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "========Property 1:========\nT(u1+u2)\n[2 3]\nT(u1)+T(u2)\n[1 3]\n========Property 2:========\nT(au1)\n[4 5]\naT(u1)\n[0 5]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Rotation Functions"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import math\ndef rotateTransform(vector2d):\n    return np.array([vector2d[0]*math.cos(math.radians(60)),vector2d[1]*math.cos(math.radians(60))])",
      "execution_count": 128,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "u = np.array([1,1])\nv = np.array([2,2])\nprint(\"========Property 1:========\")\nprint(\"T(u1+u2)\")\nprint(rotateTransform(u+v))\nprint(\"T(u1)+T(u2)\")\nprint(rotateTransform(u)+rotateTransform(v))\nprint(\"========Property 2:========\")\nprint(\"T(au1)\")\nprint(rotateTransform(5*u))\nprint(\"aT(u1)\")\nprint(5*rotateTransform(u))",
      "execution_count": 129,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "========Property 1:========\nT(u1+u2)\n[ 1.5  1.5]\nT(u1)+T(u2)\n[ 1.5  1.5]\n========Property 2:========\nT(au1)\n[ 2.5  2.5]\naT(u1)\n[ 2.5  2.5]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Matrices\nA matrix is a 2-d array of elements. It is used to conviniently represent sets of data.  \nHere is an example of a matrix:  \n\\begin{bmatrix}\n    {1}       & {4} & {7} \\\\\n    {2}       & {5} & {8} \\\\\n    {3}       & {6} & {9} \\\\\n\\end{bmatrix}  \nThe rows are the horizontal lines and the columns are the vertical line of numbers.  \nThe size of a matrix is the number of rows by the number of columns. Here the size is 3x3.  \nFormally, a m rows and n column matrix are represented as  \n$$ A = (a_{ij})_{m,n} $$ \nThe i,j is denotes the row and column of a matrix.   \nThere are multiple ways of creating matrices in numpy. You can create an 2-d array or directly invoke the matrix api. We would stick with creating matrices using the array api."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "A = np.array([[1,4,7], [2, 5,8],[3,6,9]])\nprint(A)",
      "execution_count": 140,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[1 4 7]\n [2 5 8]\n [3 6 9]]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's access some elements using the index notation:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(A[0][0])\nprint(A[1][2])\nprint(A[2][2])",
      "execution_count": 141,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "1\n8\n9\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Matrix Vector Product (MVP)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's say A is an mxn matrix with columns A1,A2,A3...An. Then if u is a vector of size n , the matrix vector product is defined as "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "$$\nAu = A_1u_1+A_2u_2+A_3u_3+....+A_nu_n\n$$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Example:    "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "$$\nA \\ = \\ \\begin{bmatrix}\n    {1}       & {4} & {7} \\\\\n    {2}       & {5} & {8} \\\\\n    {3}       & {6} & {9} \\\\\n\\end{bmatrix}\n\\ and \\ u = \\\n\\begin{bmatrix}\n    {1} \\\\\n    {2} \\\\\n    {3} \\\\\n\\end{bmatrix}\n$$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "$$\n= \n1\n\\begin{bmatrix}\n    {1} \\\\\n    {2} \\\\\n    {3} \\\\\n\\end{bmatrix}\n+\n2\n\\begin{bmatrix}\n    {4} \\\\\n    {5} \\\\\n    {6} \\\\\n\\end{bmatrix}\n+\n3\n\\begin{bmatrix}\n    {7} \\\\\n    {8} \\\\\n    {9} \\\\\n\\end{bmatrix}\n$$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "$$\n= \n\\begin{bmatrix}\n    {1} \\\\\n    {2} \\\\\n    {3} \\\\\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n    {8} \\\\\n    {10} \\\\\n    {12} \\\\\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n    {21} \\\\\n    {24} \\\\\n    {27} \\\\\n\\end{bmatrix}\n$$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "$$=\n\\begin{bmatrix}\n    {1+8+21}\\\\\n    {2+10+24}\\\\\n    {3+12+27}\\\\\n\\end{bmatrix} \n$$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "$$\n=\n\\begin{bmatrix}\n    {30}\\\\\n    {36}\\\\\n    {42}\\\\\n\\end{bmatrix} \n$$"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "A = np.array([[1,4,7], [2, 5,8],[3,6,9]])\nu = np.array([[1],[2],[3]])\nprint(np.matmul(A,u))",
      "execution_count": 150,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[30]\n [36]\n [42]]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Matrices can be used to represent a linear transform. Multiplication of a vector by a matrix performs a linear combination and thus transforms the input vector into an output vector, possibly of a different size.**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "$$ \n\\begin{bmatrix}\n    {1}       & {4} & {7} \\\\\n    {2}       & {5} & {8} \\\\\n    {3}       & {6} & {9} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n    x_{1} \\\\\n    x_{2} \\\\\n    x_{3} \\\\\n\\end{bmatrix}\n=\nx_{1}\n\\begin{bmatrix}\n    {1} \\\\\n    {2} \\\\\n    {3} \\\\\n\\end{bmatrix}\n+\nx_{2}\n\\begin{bmatrix}\n    {4} \\\\\n    {5} \\\\\n    {6} \\\\\n\\end{bmatrix}\n+\nx_{3}\n\\begin{bmatrix}\n    {7} \\\\\n    {5} \\\\\n    {6} \\\\\n\\end{bmatrix}\n$$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "$$\n= \n\\begin{bmatrix}\n    {x_{1}} \\\\\n    {2x_{1}} \\\\\n    {3x_{1}} \\\\\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n    {4x_{2}} \\\\\n    {5x_{2}} \\\\\n    {6x_{2}} \\\\\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n    {7x_{3}} \\\\\n    {8x_{3}} \\\\\n    {9x_{3}} \\\\\n\\end{bmatrix}\n$$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "$$\n= \n\\begin{bmatrix}\n    {x_{1}+4x_{2}+7x_{3}} \\\\\n    {2x_{1}+5x_{2}+8x_{3}} \\\\\n    {3x_{1}+6x_{2}+9x_{3}} \\\\\n\\end{bmatrix}\n$$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "*** Matrix operations are linear ***"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "A(x+y) = Ax + Ay  \nA(ax)  = aAx"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "##Property 1\nprint(\"Property1: A(x+y) = Ax + Ay  \")\nA = np.array([[1,4,7], [2, 5,8],[3,6,9]])\nu = np.array([[1],[2],[3]])\nv = np.array([[3],[4],[5]])\nprint(\"A(x+y)\")\nprint(np.matmul(A,u+v))\nA = np.array([[1,4,7], [2, 5,8],[3,6,9]])\nu = np.array([[1],[2],[3]])\nv = np.array([[3],[4],[5]])\nprint(\"A(x)+A(y)\")\nprint(np.matmul(A,u)+np.matmul(A,v))",
      "execution_count": 154,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Property1: A(x+y) = Ax + Ay  \nA(x+y)\n[[ 84]\n [102]\n [120]]\nA(x)+A(y)\n[[ 84]\n [102]\n [120]]\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "##Property 2\nprint(\"Property2: A(ax) = aA(x)\")\nA = np.array([[1,4,7], [2, 5,8],[3,6,9]])\nu = np.array([[1],[2],[3]])\nprint(\"A(ax)\")\nprint(np.matmul(A,3*u))\nprint(\"aA(x)\")\nprint(3*np.matmul(A,u))",
      "execution_count": 156,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Property2: A(ax) = aA(x)\nA(ax)\n[[ 90]\n [108]\n [126]]\naA(x)\n[[ 90]\n [108]\n [126]]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##### Deriving a matrix from a linear transformation\nLet's do the process in reverse now."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "$$\n\\begin{bmatrix}\n    {x_{1}+4x_{2}+7x_{3}} \\\\\n    {5x_{2}+8x_{3}} \\\\\n    {3x_{1}+9x_{3}} \\\\\n\\end{bmatrix}\n$$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "$$\n=\n\\begin{bmatrix}\n    {x_{1}} \\\\\n    {0 x_{1}} \\\\\n    {3x_{1}} \\\\\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n    {4x_{2}} \\\\\n    {5x_{2}} \\\\\n    {0x_{2}} \\\\\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n    {7x_{3}} \\\\\n    {8x_{3}} \\\\\n    {9x_{3}} \\\\\n\\end{bmatrix}\n$$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "$$\n=\nx_{1}\n\\begin{bmatrix}\n    {1} \\\\\n    {0} \\\\\n    {3} \\\\\n\\end{bmatrix}\n+\nx_{2}\n\\begin{bmatrix}\n    {4} \\\\\n    {5} \\\\\n    {0} \\\\\n\\end{bmatrix}\n+\nx_{3}\n\\begin{bmatrix}\n    {7} \\\\\n    {8} \\\\\n    {9} \\\\\n\\end{bmatrix}\n$$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "$$\n=\n\\begin{bmatrix}\n    {1}       & {4} & {7} \\\\\n    {0}       & {5} & {8} \\\\\n    {3}       & {0} & {9} \\\\\n\\end{bmatrix} \n\\begin{bmatrix}\n    {x_1}\\\\\n    {x_2}\\\\\n    {x_3}\\\\\n\\end{bmatrix} \n$$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Matrix Algebra"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Addition of matrices\nLet's get down to some operations on matrices. The sum of two matrices is the sum of corresponding entries of the matrices and only makes sense if they are of same size.Formally  \n$$ [A+B]_{ij} = [A]_{ij} + [B]_{ij}   $$"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "A = np.random.random((2, 2))\nB = np.random.random((2, 2))\nprint(\"A = \")\nprint(A)\nprint(\"B = \")\nprint(B)\nprint(\"SUM:\")\nprint(A+B)",
      "execution_count": 158,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "A = \n[[ 0.97230088  0.80779044]\n [ 0.60804372  0.07976949]]\nB = \n[[ 0.92501422  0.7275366 ]\n [ 0.83090743  0.78245292]]\nSUM:\n[[ 1.8973151   1.53532704]\n [ 1.43895115  0.86222241]]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Scalar Multiplication\nScalar multiplication will just scale individual components of A.  \n$$ [aA]_{ij} = a[A]_{ij} $$"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(3*A)",
      "execution_count": 170,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[ 2.91690265  2.42337132]\n [ 1.82413115  0.23930846]]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Transpose of a matrix\nThe transpose of a matrix interchanges the row and columns of a matrix. It is expressed as  \n$$ A_{ij}^T = A_{ji}  $$"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(A.T)",
      "execution_count": 172,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[ 0.97230088  0.60804372]\n [ 0.80779044  0.07976949]]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##### Symmetric Matrices\nIf the transpose of a matrix is equal to the matrix, such matrices are known as symmetric matrices.\n$$  A^T = A $$"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "I = np.eye(3)\nprint(I)\nIT = I.T\nprint(IT)\nprint(I==IT)",
      "execution_count": 174,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[ 1.  0.  0.]\n [ 0.  1.  0.]\n [ 0.  0.  1.]]\n[[ 1.  0.  0.]\n [ 0.  1.  0.]\n [ 0.  0.  1.]]\n[[ True  True  True]\n [ True  True  True]\n [ True  True  True]]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Theorms on symmetric matrices:  \n1) symmetric matrices are square."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Matrix multiplication\nMultiplying matrices are very different than one might initially anticipate. And the difference arises because matrix multiplication denotes the idea of composing (or chaining) linear transformations. We can multiply matrices of different shapes and end-up with a matrix of other shape."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "For simplicity, let's consider two  2x2 matrices u and v as :"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "$$  \nu = \\begin{bmatrix}\n    {1}  & {3} \\\\\n    {2}  & {4}  \\\\\n\\end{bmatrix} \n\\ v = \\begin{bmatrix}\n    {5}  & {7} \\\\\n    {6}  & {8}  \\\\\n\\end{bmatrix} \n$$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "then, the matrix multiplication u*v is defined as :"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "$$\n\\begin{bmatrix}\n    {1}  & {3} \\\\\n    {2}  & {4}  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n    {5} \\\\\n    {6}\n\\end{bmatrix}\n|\n\\begin{bmatrix}\n    {1}  & {3} \\\\\n    {2}  & {4}  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n    {7} \\\\\n    {8}\n\\end{bmatrix}\n$$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "$$\n= \n\\begin{bmatrix}\n    { 1x5 + 3x6} \\\\\n    { 2x5 + 4x6} \n\\end{bmatrix}\n|\n\\begin{bmatrix}\n    { 1x7 + 3x8} \\\\\n    { 2x7 + 4x8} \n\\end{bmatrix}\n$$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "$$ =\n\\begin{bmatrix}\n    {23}  & {31} \\\\\n    {34}  & {46}  \\\\\n\\end{bmatrix}\n$$"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "u = np.array([[1,3],[2,4]])\nv = np.array([[5,7],[6,8]])\nprint(np.matmul(u, v))",
      "execution_count": 168,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[23 31]\n [34 46]]\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(np.matmul(A, B))",
      "execution_count": 169,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[ 1.57059123  1.33944246]\n [ 0.62873015  0.50478992]]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Another way to describe the matrix multiplication operation is that it's a dot product of row and column vectors. Another important thing to remember is that the operation only defined if number of columns of matrix on the left  matches with number of rows on the matrix to the right of the operator. The result of the multipication is a matrix with same number of rows as the left matrix by number of columns on the right. i.e  let's say u is mxn and v is nxp , then the output will be of shape mxp."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It's worth spending some time doing few examples to sink it in and practice multiplying few 2x2 and 3x3 matrices and also some rectangular matrices. Also observe that as the dimension grows, it becomes more tedious to carry out multiplication."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### * STOP AND PRACTICE *"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now if you have done some examples and felt that multiplying 3x3 is a lot worse than 2x2, think about multiplying 10000x10000 matrices, or millionxmillion matrices. These usecases are not fictional and are often required.  \nThere are some methods developed which can break a big matrix into sub-matrices (blocks) and perform multiplication in parallel , and then combine the result, but we are not going to cover those here. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Properties of matrix multiplication\n1) Matrix multiplication are **not** commutative. AB != BA. It might even be the case that AxB is defined whereas BxA is not defined.  \n2) Multiplication distributes over addition. A(B+C) = AB+AC  \n3)$$ (AB)^T = B^TA^T   $$\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Inverse of a Matrix"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "Ainv = np.linalg.inv(A)\nprint(Ainv)",
      "execution_count": 176,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[-0.19286069  1.9530152 ]\n [ 1.47008253 -2.35075621]]\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(np.matmul(A, Ainv))",
      "execution_count": 177,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[  1.00000000e+00   0.00000000e+00]\n [ -8.32667268e-17   1.00000000e+00]]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Linear Equations\nA linear equation is an equation of the form  \n$$ a_1x_1+a_2x_2+a_3x_3+.....+a_1x_n = c_1 $$\n where  there are n variables  $$ x_1,x_2 ... x_n $$ and $$a_1,a_2 .. c_1 $$ are known constant  \n So the solution of the equations is ordered list of numbers s1,s2,s3...sn that when substituted satisfies the linear equation.  \n\n##### System of linear equations\nThe graph of a linear equations is a line. In a system of linear equations, there are multiple lines and an interesting question to ask for is where does the line intersect. In order words, what are the solutions which satisfies all the linear equations in a system of equations. There can be 3 possible answers to above questions:  \n1) The lines are parallel .. they do not intersect and thus have no solution.Such systems are also called singular or inconsistent.  \n2) The lines are same .. there are infinite number of solutions  \n3) The lines intersect have exactly one solution  "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "For example, consider system of linear equations:  \n$$ a_1x_1+b_1x_2+c_1x_3 = d_1 $$\n$$ a_2x_1+b_2x_2+c_2x_3 = d_2 $$\n$$ a_3x_1+b_3x_2+c_3x_3 = d_3 $$  \nWe can represent the same information with matrix as show below:  \n$$ \n\\begin{bmatrix}\n    a_{1}       & b_{1} & c_{1} \\\\\n    a_{2}       & b_{2} & c_{2} \\\\\n    a_{3}       & b_{3} & c_{3} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n    x_{1} \\\\\n    x_{2} \\\\\n    x_{3} \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n    d_{1} \\\\\n    d_{2} \\\\\n    d_{3} \\\\\n\\end{bmatrix}\n$$\n\nAnd we can describe the above form in general as   \n$$ Ax=b $$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Matrix Factorization\nIn matrix factorization, we are interested in finding two or more matrices whose product gives you the original matrix. This process is also called as matrix decomposition.  \nThere are numerous matrix factorization algorithms,like LU,QR,Rank, Cholskey etc and each algorithm breaks the matrix such that the decomposed matrices solve a particular class of problems well.  \nWikipedia has good article which gives overview of all the different techniques:  \nhttps://en.wikipedia.org/wiki/Matrix_decomposition"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import scipy\nP, L, U = scipy.linalg.lu(A)\n\nprint(\"A:\")\nprint(A)\n\nprint(\"P:\")\nprint(P)\n\nprint(\"L:\")\nprint(L)\n\nprint(\"U:\")\nprint(U)",
      "execution_count": 36,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "A:\n[[ 0.56073128  0.95912425  0.18109907]\n [ 0.97114416  0.44711371  0.73957315]\n [ 0.81032431  0.50232417  0.38825604]]\nP:\n[[ 0.  1.  0.]\n [ 1.  0.  0.]\n [ 0.  0.  1.]]\nL:\n[[ 1.          0.          0.        ]\n [ 0.57739242  1.          0.        ]\n [ 0.83440168  0.18439136  1.        ]]\nU:\n[[ 0.97114416  0.44711371  0.73957315]\n [ 0.          0.70096418 -0.24592486]\n [ 0.          0.         -0.18349862]]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Determinant\nDeterminant is a function that operates on square matrices to produce a scalar. Determinants maps to area spanned by two vectors in case of 2x2 matrices and to volume in case of 3x3 matrices.  \nhttp://www.maths.manchester.ac.uk/~lwalker/MATH10000/project-04-part2.pdf  \nA 2×2 determinant is defined to be\n$$ det[a b; c d]=|a b; c d|=ad-bc.  $$"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(np.linalg.det(A))",
      "execution_count": 69,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "-0.285374711663\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##### Eigen Values and Eigen Vectors\nEigen is German means \"characteristic\" or \"individual\". Eigen vectors and eigen values brings out important characteristics of a matrix. Just by looking at eigen values and vectors of a matrix, you can tell a lot about the nature of the transformation.   \nBy definition, for a given nxn matrix A, then a non-zero vector x is said to be the eigen vector of A if Ax is a scalar multiple of x. i.e.  \n$$ Ax = \\lambda x $$\nHere lambda is a scalar value."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##### Computing eigen values\nRewriting the definition, the equation can be written as :  \n$$ (A-\\lambda ) x =0 $$\nAs x by definition cannot be 0,   \n$$ det(A-\\lambda) = 0 $$\nThe above equation is called characteristic equation of A.  "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "w, v = np.linalg.eig(A)\nprint(w)\nprint(v)",
      "execution_count": 38,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[ 1.86507234+0.j         -0.23448566+0.10950836j -0.23448566-0.10950836j]\n[[ 0.55077013+0.j          0.69697853+0.j          0.69697853-0.j        ]\n [ 0.65018887+0.j         -0.48658111+0.11281292j -0.48658111-0.11281292j]\n [ 0.52336097+0.j         -0.48347780-0.17601764j -0.48347780+0.17601764j]]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Application of eigen:\nPowers of matrix A.\nDiagonalization.\nTransition matrix."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Positive Definite and Positive-Semi definate matrices"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def is_pos_def(x):\n    return np.all(np.linalg.eigvals(x) > 0)\n\nprint(is_pos_def(A))",
      "execution_count": 39,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "False\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Singular Value Decomposition"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": " U, s, V = np.linalg.svd(A, full_matrices=True)\nprint(\"A:\")\nprint(A)\n\nprint(\"U:\")\nprint(U)\n\nprint(\"s:\")\nprint(s)\n\nprint(\"V:\")\nprint(V)",
      "execution_count": 40,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "A:\n[[ 0.56073128  0.95912425  0.18109907]\n [ 0.97114416  0.44711371  0.73957315]\n [ 0.81032431  0.50232417  0.38825604]]\nU:\n[[-0.53157978  0.81264395 -0.2388153 ]\n [-0.65598447 -0.57335562 -0.49086425]\n [-0.53582396 -0.10427439  0.83786606]]\ns:\n[ 1.91000513  0.59726374  0.10949938]\nV:\n[[-0.71691958 -0.56141631 -0.41332559]\n [-0.31080432  0.78808035 -0.53134737]\n [ 0.62404086 -0.25246996 -0.73948085]]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Rank of a matrix"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(np.linalg.matrix_rank(A))",
      "execution_count": 41,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "3\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Constructing orthonormal basis "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Construct an orthonormal basis for the range of A using SVD\nprint(scipy.linalg.orth(A))",
      "execution_count": 42,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[-0.53157978  0.81264395 -0.2388153 ]\n [-0.65598447 -0.57335562 -0.49086425]\n [-0.53582396 -0.10427439  0.83786606]]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##### Find null space of a matrix"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def nullspace(A, atol=1e-13, rtol=0):\n    A = np.atleast_2d(A)\n    u, s, vh = np.linalg.svd(A)\n    tol = max(atol, rtol * s[0])\n    nnz = (s >= tol).sum()\n    ns = vh[nnz:].conj().T\n    return ns\nprint(nullspace(A))",
      "execution_count": 43,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Reduced Row Echleon Form:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "## Adapted from https://rosettacode.org/wiki/Reduced_row_echelon_form\ndef ReducedRowEchelonForm(M):\n    if M.size==0: return\n    lead = 0\n    rowCount = len(M)\n    columnCount = len(M[0])\n    for r in range(rowCount):\n        if lead >= columnCount:\n            return\n        i = r\n        while M[i][lead] == 0:\n            i += 1\n            if i == rowCount:\n                i = r\n                lead += 1\n                if columnCount == lead:\n                    return\n        M[i],M[r] = M[r],M[i]\n        lv = M[r][lead]\n        M[r] = [ mrx / float(lv) for mrx in M[r]]\n        for i in range(rowCount):\n            if i != r:\n                lv = M[i][lead]\n                M[i] = [ iv - lv*rv for rv,iv in zip(M[r],M[i])]\n        lead += 1",
      "execution_count": 70,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "ReducedRowEchelonForm(B)\nprint(B)",
      "execution_count": 72,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[ 1.  0.  0.]\n [ 0.  1.  0.]\n [ 0.  0.  1.]]\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Linear Span  \nLinear Independence and Basis vectors  \nAffine, conical, and convex combinations  \nFundamental Spaces  "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "For example, consider system of linear equations:  \n$$ a_1x_1+b_1x_2+c_1x_3 = d_1 $$\n$$ a_2x_1+b_2x_2+c_2x_3 = d_2 $$\n$$ a_3x_1+b_3x_2+c_3x_3 = d_3 $$  \nWe can represent the same information with matrix as show below:  \n$$ \n\\begin{bmatrix}\n    a_{1}       & b_{1} & c_{1} \\\\\n    a_{2}       & b_{2} & c_{2} \\\\\n    a_{3}       & b_{3} & c_{3} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n    x_{1} \\\\\n    x_{2} \\\\\n    x_{3} \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n    d_{1} \\\\\n    d_{2} \\\\\n    d_{3} \\\\\n\\end{bmatrix}\n$$"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}